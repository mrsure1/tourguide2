name: Scheduled Scraper

on:
  schedule:
    # 매일 12:00 KST (UTC 03:00)과 18:00 KST (UTC 09:00)에 실행
    - cron: '0 3,9 * * *'
  workflow_dispatch: # 수동 실행 가능

permissions:
  contents: write

jobs:
  scrape-and-update:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
        cache: 'pip'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
        
    - name: Run Full Pipeline
      env:
        # GitHub Repository Settings > Secrets and variables > Actions 에서 설정 필요
        SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
        SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
        GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
      run: |
        python run_full_pipeline.py
    
    - name: Commit and Push policies.json
      run: |
        git config --global user.name 'github-actions[bot]'
        git config --global user.email 'github-actions[bot]@users.noreply.github.com'
        
        # policies.json 파일이 변경되었는지 확인하고 커밋
        git add policies.json
        git diff --quiet && git diff --staged --quiet || (git commit -m "Update policies.json from scheduled scraper [skip ci]" && git push)
